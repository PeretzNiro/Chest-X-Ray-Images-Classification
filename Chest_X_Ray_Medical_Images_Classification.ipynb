{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyJk05ReW1w0"
      },
      "source": [
        "# PneumoNet: Empowering Healthcare with AI-driven Pneumonia Detection #\n",
        "\n",
        "**<font size=\"3\">Early and accurate diagnosis in modern healthcare plays a pivotal role in improving patient outcomes. Our project, \"PneumoNet,\" harnesses the power of artificial intelligence to revolutionize the detection of pneumonia from chest X-rays. Pneumonia, a prevalent respiratory condition, requires prompt identification for timely intervention and optimal patient care.\n",
        "Our project utilizes a diverse dataset comprising chest X-ray images from reputable healthcare repositories. The dataset encompasses normal and pneumonia-affected cases, ensuring a robust model capable of discerning subtle radiological patterns indicative of the disease. Leveraging advanced deep learning techniques, we aim to develop an intelligent diagnostic tool capable of swiftly and accurately identifying pneumonia in chest X-ray images.\n",
        "The significance of our project lies in its potential to expedite diagnosis, reduce the workload on healthcare professionals, and enhance overall healthcare efficiency. With the integration of our AI-driven diagnostic solution, we aspire to facilitate timely treatment initiation, thereby improving patient outcomes and alleviating the strain on healthcare resources. PneumoNet represents a leap towards enhancing diagnostic capabilities and ultimately contributing to advancing healthcare practices.</font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKlzSa2zWns6"
      },
      "source": [
        "## Importing necessary libraries ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1elOSj7JUAu_"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import imageio\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnBaqjoGw3uV"
      },
      "source": [
        "## Oversampling & Blancing the dataset - Augmented Dataset ##\n",
        "\n",
        "**<font size=\"3\">Generating augmented images from the normal class to balance the training dataset. This ensures diversity in the augmented dataset by applying different data augmentation to each halves of the minority class. This helps prevent the model from overfitting to a specific set of augmented images and improves its ability to generalize to new, unseen data.**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMRWMBymw3fP"
      },
      "outputs": [],
      "source": [
        "# Generate augmented images\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Specify the directory name and its location\n",
        "desired_directory = \"/ChestXRay2017/chest_xray/train/NORMAL\"\n",
        "\n",
        "# Check if the directory already exists\n",
        "if not os.path.exists(desired_directory):\n",
        "    # If it doesn't exist, create the directory\n",
        "    os.makedirs(desired_directory)\n",
        "    print(f\"Directory '{desired_directory}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{desired_directory}' already exists.\")\n",
        "\n",
        "# Example: Specify the directory containing your images\n",
        "image_directory = \"/ChestXRay2017/chest_xray/train/NORMAL/\"\n",
        "\n",
        "# Get a list of all image file names in the directory\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith(('.jpeg'))]\n",
        "\n",
        "# Load the images using OpenCV\n",
        "images = [cv2.imread(os.path.join(image_directory, f)) for f in image_files]\n",
        "\n",
        "# Convert the images to NumPy arrays\n",
        "image_arrays = np.array(images)\n",
        "\n",
        "# Define your data augmentation parameters for the first half\n",
        "datagen_first_half = ImageDataGenerator(\n",
        "    rotation_range=8,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    shear_range=0.15,\n",
        "    zoom_range=0.15,\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=False\n",
        ")\n",
        "\n",
        "# Define your data augmentation parameters for the second half\n",
        "datagen_second_half = ImageDataGenerator(\n",
        "    rotation_range=13,  # Adjusted rotation\n",
        "    width_shift_range=0.1,  # Adjusted width shift\n",
        "    height_shift_range=0.1,  # Adjusted height shift\n",
        "    shear_range=0.25,  # Adjusted shear\n",
        "    zoom_range=0.25,  # Adjusted zoom\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=False\n",
        ")\n",
        "\n",
        "# Specify the desired total number of augmented images\n",
        "desired_total_images = 2062\n",
        "batch_size = 16\n",
        "epochs = desired_total_images // batch_size\n",
        "first_half_count = 0\n",
        "second_half_count = 0\n",
        "\n",
        "# Assuming num_batches_per_epoch is calculated based on the original dataset size\n",
        "num_batches_per_epoch = len(image_arrays) // batch_size\n",
        "\n",
        "# Calculate the number of images to generate with slightly different data augmentation\n",
        "num_slightly_different = batch_size // 2\n",
        "\n",
        "# Counter to keep track of the current original image index\n",
        "generated_images_count = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in range(num_batches_per_epoch):\n",
        "\n",
        "        # Generate and save augmented normal images\n",
        "        for i in range(batch_size):\n",
        "            # Break the loop if the desired number of augmented images is reached\n",
        "            if generated_images_count >= desired_total_images:\n",
        "                break\n",
        "\n",
        "            # Choose different data augmentation for at least half of the images\n",
        "            if i < num_slightly_different:\n",
        "                augmented_image = datagen_first_half.random_transform(image_arrays[i])\n",
        "                first_half_count += 1\n",
        "            else:\n",
        "                augmented_image = datagen_second_half.random_transform(image_arrays[i - num_slightly_different])\n",
        "                second_half_count += 1\n",
        "\n",
        "            # Update the count for each augmented image\n",
        "            generated_images_count += 1\n",
        "\n",
        "            # Save each augmented image with a unique filename\n",
        "            save_path = os.path.join(\"/ChestXRay2017/chest_xray/train/NORMAL\", f\"IM-{generated_images_count}.jpeg\")\n",
        "            plt.imsave(save_path, augmented_image)\n",
        "print(f\"{first_half_count} images created with the first data augmentation\")\n",
        "print(f\"{second_half_count} images created with the second data augmentation\")\n",
        "print(f\"{second_half_count + first_half_count} images created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZBw7FRhz7A7"
      },
      "source": [
        "## Verify all normal images are unique ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrQi2T5fz6c8"
      },
      "outputs": [],
      "source": [
        "# Check the number of unique images in a directory\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def count_unique_images(directory_path):\n",
        "    # Initialize a set to store unique image hashes\n",
        "    unique_image_hashes = set()\n",
        "\n",
        "    # Iterate through files in the directory\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "        # Check if the file is an image\n",
        "        if os.path.isfile(file_path) and any(file_path.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
        "            try:\n",
        "                # Open the image and calculate its hash\n",
        "                with Image.open(file_path) as img:\n",
        "                    image_hash = hash(img.tobytes())\n",
        "                    unique_image_hashes.add(image_hash)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    # Return the count of unique images\n",
        "    return len(unique_image_hashes)\n",
        "\n",
        "# Replace 'your_directory_path' with the path to your image directory\n",
        "directory_path = '/ChestXRay2017/chest_xray/train/NORMAL'\n",
        "unique_image_count = count_unique_images(directory_path)\n",
        "\n",
        "print(f\"Number of unique images in {directory_path}: {unique_image_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ys_tgJcWHf-"
      },
      "source": [
        "## Dataset File Paths and Summary Setup for Chest X-ray Analysis ##\n",
        "\n",
        "**<font size=\"3\">Set up file paths for a chest X-ray dataset stored in different directories for training, testing, and validation. Then creates lists of file paths for images in categories like pneumonia and normal for each dataset split (train, test, and val). Finally, it prints the total count of images in each category to provide a quick summary of the dataset.**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1HEVR1WUvLS"
      },
      "outputs": [],
      "source": [
        "# Define the base directory for the chest X-ray dataset\n",
        "base_dir = '/ChestXRay2017/chest_xray/'\n",
        "\n",
        "# Define directories for training data\n",
        "train_pneumonia_dir = base_dir + 'train/PNEUMONIA/'\n",
        "train_normal_dir = base_dir + 'train/NORMAL/'\n",
        "\n",
        "# Define directories for testing data\n",
        "test_pneumonia_dir = base_dir + 'test/PNEUMONIA/'\n",
        "test_normal_dir = base_dir + 'test/NORMAL/'\n",
        "\n",
        "# Define directories for validation data\n",
        "val_normal_dir = base_dir + 'val/NORMAL/'\n",
        "val_pneumonia_dir = base_dir + 'val/PNEUMONIA/'\n",
        "\n",
        "# Create lists of file paths for images in each category\n",
        "train_pn = [train_pneumonia_dir + \"{}\".format(i) for i in os.listdir(train_pneumonia_dir)]\n",
        "train_normal = [train_normal_dir + \"{}\".format(i) for i in os.listdir(train_normal_dir)]\n",
        "\n",
        "test_normal = [test_normal_dir + \"{}\".format(i) for i in os.listdir(test_normal_dir)]\n",
        "test_pn = [test_pneumonia_dir + \"{}\".format(i) for i in os.listdir(test_pneumonia_dir)]\n",
        "\n",
        "val_pn = [val_pneumonia_dir + \"{}\".format(i) for i in os.listdir(val_pneumonia_dir)]\n",
        "val_normal = [val_normal_dir + \"{}\".format(i) for i in os.listdir(val_normal_dir)]\n",
        "\n",
        "# Print total counts of images in different categories\n",
        "print(\"Total images:\", len(train_pn + train_normal + test_normal + test_pn + val_pn + val_normal))\n",
        "print(\"Total pneumonia images:\", len(train_pn + test_pn + val_pn))\n",
        "print(\"Total Normal images:\", len(train_normal + test_normal + val_normal))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBZr9FanZ4Nm"
      },
      "source": [
        "## Dataset Preprocessing & Visualization - Original Dataset ##\n",
        "**<font size=\"3\">The dataset is relatively small, consisting of 5873 chest X-ray images with pneumonia and normal conditions. To maximize the utility of the limited data, we adopt an 80-15-5 split strategy, dividing the dataset into training, testing, and validation sets. This distribution ensures that a substantial portion (80%) is allocated to training, allowing the model to learn patterns effectively. The 15% reserved for testing is an independent benchmark to evaluate the model's generalization performance. The remaining 5% set aside for validation aids in fine-tuning the model and preventing overfitting. This partitioning strategy balances training with available data and rigorously assesses the model's performance on unseen instances, considering the constraints posed by the dataset's size. </font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCEdt8duZ5Zi"
      },
      "outputs": [],
      "source": [
        "# Dataset Splitting (train 80%, test 15%, and validation 5%)\n",
        "\n",
        "# Combining pneumonia and normal chest X-ray images into two Python lists\n",
        "pn = train_pn + test_pn + val_pn\n",
        "normal = train_normal + test_normal + val_normal\n",
        "\n",
        "# Splitting the dataset into train set, test set, and validation set\n",
        "train_imgs = pn[:3418] + normal[:1224]  # 80% of 4642 Pneumonia and normal chest X-ray are 3418 and 1224, respectively.\n",
        "test_imgs = pn[3418:4059] + normal[1224:1502] # 15% of 919 Pneumonia and normal chest X-ray are 641 and 278, respectively.\n",
        "val_imgs = pn[4059:] + normal[1502:] # 5% of 312 Pneumonia and normal chest X-ray are 223 and 89, respectively.\n",
        "\n",
        "# Displaying the distribution of images in each set\n",
        "print(\"Total Train Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(train_imgs), len(pn[:3418]), len(normal[:1224])))\n",
        "print(\"Total Test Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(test_imgs), len(pn[3418:4059]), len(normal[1224:1502])))\n",
        "print(\"Total Validation Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(val_imgs), len(pn[4059:]), len(normal[1502:])))\n",
        "\n",
        "# Randomly shuffling the images in each set\n",
        "import random\n",
        "random.shuffle(train_imgs)\n",
        "random.shuffle(test_imgs)\n",
        "random.shuffle(val_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTyHFGUH2NFI"
      },
      "source": [
        "## Dataset Preprocessing & Visualization - Augmented Dataset ##\n",
        "**<font size=\"3\">The dataset is relatively small, consisting of 8529 chest X-ray images with pneumonia and normal conditions. To maximize the utility of the limited data, we adopt an 86-13-1 split strategy, dividing the dataset into training, testing, and validation sets. This distribution ensures that a substantial portion (86%) is allocated to training, allowing the model to learn patterns effectively. The 13% reserved for testing is an independent benchmark to evaluate the model's generalization performance. The remaining 1% set aside for validation aids in fine-tuning the model and preventing overfitting. This partitioning strategy balances training with available data and rigorously assesses the model's performance on unseen instances, considering the constraints posed by the dataset's size.</font>**\n",
        "\n",
        "**<font size=\"3\">The distribution of the data could potentially be balanced differently, but this adjustment wasn't implemented in this case due to time constraints.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiCu2_Bs2L4a"
      },
      "outputs": [],
      "source": [
        "# Dataset Splitting (train 86%, test 13%, and validation 1%)\n",
        "\n",
        "# Combining pneumonia and normal chest X-ray images into two Python lists\n",
        "pn = train_pn + test_pn + val_pn\n",
        "normal = train_normal + test_normal + val_normal\n",
        "\n",
        "# Splitting the dataset into train set, test set, and validation set\n",
        "train_imgs = pn[:3411] + normal[:3411]  # 86% of 6822 Pneumonia and normal chest X-ray are 3411 and 3411, respectively.\n",
        "test_imgs = pn[3411:4222] + normal[3411:3615] # 13% of 919 Pneumonia and normal chest X-ray are 811 and 204, respectively.\n",
        "val_imgs = pn[4222:] + normal[3615:] # 1% of 312 Pneumonia and normal chest X-ray are 60 and 38, respectively.\n",
        "\n",
        "# Displaying the distribution of images in each set\n",
        "print(\"Total Train Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(train_imgs), len(pn[:3411]), len(normal[:3411])))\n",
        "print(\"Total Test Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(test_imgs), len(pn[3411:4222]), len(normal[3411:3615])))\n",
        "print(\"Total Validation Images: %s containing %s pneumonia and %s normal images\"\n",
        "      % (len(val_imgs), len(pn[4222:]), len(normal[3615:])))\n",
        "\n",
        "# Randomly shuffling the images in each set\n",
        "import random\n",
        "random.shuffle(train_imgs)\n",
        "random.shuffle(test_imgs)\n",
        "random.shuffle(val_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ7V0pqugJVc"
      },
      "source": [
        "## Preprocess the images ##\n",
        "**<font size=\"3\">The purpose of this code is to prepare a dataset for training a machine learning model on chest X-ray images, ensuring uniformity in size, color channels, and labeling for further analysis and model development. The preprocessing steps help create a consistent and standardized input for a machine learning algorithm. </font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33VaD6UmgJlw"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "img_size = 224\n",
        "\n",
        "def preprocess_image(image_list):\n",
        "    \"\"\"\n",
        "    Preprocesses a list of image file paths.\n",
        "\n",
        "    Args:\n",
        "    - image_list (list): List of file paths for images.\n",
        "\n",
        "    Returns:\n",
        "    - X (list): Processed images.\n",
        "    - y (list): Labels (0 for Normal or 1 for Pneumonia).\n",
        "    \"\"\"\n",
        "\n",
        "    X = [] # Images\n",
        "    y = [] # Labels (0 for Normal or 1 for Pneumonia)\n",
        "    count = 0\n",
        "\n",
        "    for image in image_list:\n",
        "\n",
        "        try:\n",
        "\n",
        "            # Read and convert the image\n",
        "            img = cv2.imread(image, cv2.IMREAD_GRAYSCALE) # Read the image in grayscale\n",
        "            img = cv2.resize(img, (img_size, img_size), interpolation=cv2.INTER_CUBIC) # Resize the image to the target size\n",
        "            img = np.dstack([img, img, img]) # Convert the grayscale image to a 3D RGB image\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert the image from BGR to RGB color space. This is done to ensure the image has three channels (RGB).\n",
        "\n",
        "            # Normalalize Image\n",
        "            img = img.astype(np.float32)/255.\n",
        "\n",
        "            count += 1\n",
        "            X.append(img)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # Get the labels\n",
        "        if 'NORMAL' in image or 'IM' in image:\n",
        "            y.append(0)\n",
        "        elif 'virus' in image or 'bacteria' in image:\n",
        "            y.append(1)\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tKzqz1M0V2U"
      },
      "source": [
        "## Preprocess train images ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLTh2RIk0WMT"
      },
      "outputs": [],
      "source": [
        "# get the labels for train set\n",
        "X, y = preprocess_image(train_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md4Q_QQB1qn-"
      },
      "source": [
        "## Check if all train images getting labels ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGX4uciP1q2g"
      },
      "outputs": [],
      "source": [
        "arr = y\n",
        "# Get a tuple of unique values & their frequency in numpy array\n",
        "uniqueValues, occurCount = np.unique(arr, return_counts=True)\n",
        "\n",
        "# Informative printing labels alongside their occurrence counts.\n",
        "for label, count in zip(uniqueValues, occurCount):\n",
        "    print(f\"Label {label}: Occurs {count} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYqPi7LxMy"
      },
      "source": [
        "## Display some images from train set ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pABSOopPLxny"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "k = 1\n",
        "for i in range(4):\n",
        "    a = fig.add_subplot(1, 4, k)\n",
        "    if (y[i] == 0):\n",
        "        a.set_title('Normal')\n",
        "    else:\n",
        "        a.set_title('Pneumonia')\n",
        "\n",
        "    plt.imshow(X[i])\n",
        "    k += 1;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNl58IvcL18A"
      },
      "source": [
        "## Preprocess test images ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP40lvPRL2dw"
      },
      "outputs": [],
      "source": [
        "# get the labels for test set\n",
        "P, t = preprocess_image(test_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUHSj3dbL4UF"
      },
      "source": [
        "## Check if all test images getting labels ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vxVQrZFL3DG"
      },
      "outputs": [],
      "source": [
        "arr = t\n",
        "# Get a tuple of unique values & their frequency in numpy array\n",
        "uniqueValues, occurCount = np.unique(arr, return_counts=True)\n",
        "\n",
        "# Informative printing labels alongside their occurrence counts.\n",
        "for label, count in zip(uniqueValues, occurCount):\n",
        "    print(f\"Label {label}: Occurs {count} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y9PQBNHL7Fu"
      },
      "source": [
        "## Display some images from test set ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aCFb-voL7cG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "k = 1\n",
        "for i in range(4):\n",
        "    a = fig.add_subplot(1, 4, k)\n",
        "    if (t[i] == 0):\n",
        "        a.set_title('Normal')\n",
        "    else:\n",
        "        a.set_title('Pneumonia')\n",
        "\n",
        "    plt.imshow(P[i])\n",
        "    k += 1;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT4-cFfTL9g0"
      },
      "source": [
        "## Preprocess validation images ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg6ardvZL95h"
      },
      "outputs": [],
      "source": [
        "# get the labels for validation set\n",
        "K, m = preprocess_image(val_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhl0rqO4L_44"
      },
      "source": [
        "## Check if all validation images getting labels ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odo6-cIvMBL9"
      },
      "outputs": [],
      "source": [
        "arr = m\n",
        "# Get a tuple of unique values & their frequency in numpy array\n",
        "uniqueValues, occurCount = np.unique(arr, return_counts=True)\n",
        "\n",
        "# Informative printing labels alongside their occurrence counts.\n",
        "for label, count in zip(uniqueValues, occurCount):\n",
        "    print(f\"Label {label}: Occurs {count} times\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTMORfKFMDUA"
      },
      "source": [
        "## Display some images from validation set ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9Bb4J5tMDm2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "k = 1\n",
        "for i in range(4):\n",
        "    a = fig.add_subplot(1, 4, k)\n",
        "    if (m[i] == 0):\n",
        "        a.set_title('Normal')\n",
        "    else:\n",
        "        a.set_title('Pneumonia')\n",
        "\n",
        "    plt.imshow(K[i])\n",
        "    k += 1;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRkJk4l3upLq"
      },
      "source": [
        "## Understanding the balance or imbalance of data in different datasets ##\n",
        "**<font size=\"3\">Creates a DataFrame from three arrays and visualizes the distribution of data across the 'Train', 'Test', and 'Val' datasets using countplots.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAl_slpTup5-"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame()\n",
        "df['Train'] = y\n",
        "df['Test'] = pd.Series(t)\n",
        "df['Val'] = pd.Series(m)\n",
        "\n",
        "data_size = 3500\n",
        "\n",
        "# Create subplots\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 7))\n",
        "\n",
        "# Adjust countplot parameters and set y-axis limits\n",
        "sns.countplot(x='Train', data=df, ax=ax[0])\n",
        "ax[0].set_ylim(0, data_size)\n",
        "\n",
        "sns.countplot(x='Test', data=df, ax=ax[1])\n",
        "ax[1].set_ylim(0, data_size)\n",
        "\n",
        "sns.countplot(x='Val', data=df, ax=ax[2])\n",
        "ax[2].set_ylim(0, data_size)\n",
        "\n",
        "# Display the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6zn2ejJO2br"
      },
      "source": [
        "## Dealing with class imbalance with class weights - Use only for the original dataset ##\n",
        "\n",
        "**<font size=\"3\">In imbalanced datasets, where certain classes have significantly fewer samples than others, models might be biased towards the majority class during training. Assigning class weights helps the model give more importance to the minority class, ensuring that the model is not dominated by the majority class. By using class weights during training, the model is encouraged to pay more attention to the underrepresented classes, leading to a more balanced and fair learning process.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk4xG35BO2zx"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                 classes=np.unique(y), # here, y contains train set label\n",
        "                                                 y=y)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxFiAIGVATQl"
      },
      "source": [
        "## Prepares and processes image data ##\n",
        "\n",
        "**<font size=\"3\">Prepare and processes image data for a machine learning model by combining two sets of images and converting the images and labels into NumPy arrays.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI3SmgOeATn3"
      },
      "outputs": [],
      "source": [
        "# Convert the processed images and labels into NumPy arrays\n",
        "X_train = np.array(X)\n",
        "y_train = np.array(y)\n",
        "X_test = np.array(P)\n",
        "y_test = np.array(t)\n",
        "X_val = np.array(K)\n",
        "y_val = np.array(m)\n",
        "\n",
        "# Print the shapes of the arrays\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HzCcs71AdEz"
      },
      "source": [
        "## Training parameters ##\n",
        "\n",
        "<font size=\"3\">We will use a batch size of 16. Batch size should be a power of 2. The batch size 16 means the model will train 16 training samples and then update its parameters once. Batch training is faster and memory efficient.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZsXRTGXAdlt"
      },
      "outputs": [],
      "source": [
        "# Get the length of the train, test and validation data\n",
        "ntrain = len(X_train)\n",
        "nval = len(X_val)\n",
        "ntest = len(X_test)\n",
        "\n",
        "# Setting batch size\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ1UHGVXAtD9"
      },
      "source": [
        "## Data Augmentation ##\n",
        "**<font size=\"3\">We employ data augmentation to artificially expand our dataset, especially given its relatively small size. Mild data augmentation is applied exclusively to the training data, mirroring the real-world scenario of standardized chest X-ray images, which the machine will encounter during testing. This approach not only aids in preventing overfitting but also aligns the training process with the expected characteristics of the test data.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It2x0c8qAtkb"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation for the training set\n",
        "train_datagen = ImageDataGenerator(  rotation_range=15,\n",
        "                                     width_shift_range=0.05,\n",
        "                                     height_shift_range=0.05,\n",
        "                                     shear_range=0.2,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True)\n",
        "\n",
        "# Data augmentation for the test set\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "# Data augmentation for the validation set\n",
        "val_datagen = ImageDataGenerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LbbPTf3Ayot"
      },
      "outputs": [],
      "source": [
        "# Create the image generators\n",
        "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)\n",
        "test_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNETb32MA67u"
      },
      "outputs": [],
      "source": [
        "# Set image Size\n",
        "img_size = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdPIJqEIA9v9"
      },
      "source": [
        "## Train full train set with MobileNet  ##\n",
        "\n",
        "**<font size=\"3\"> We don't build a convolutional neural network (CNN) architecture from scratch here. Instead, we use a pretrainted CNN architecture called MobileNet. </font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A45uKWBRA_TO"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.applications import *\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# Create the base pre-trained model\n",
        "base_model = MobileNet(weights=None, include_top=False,input_shape=(img_size, img_size, 3))\n",
        "\n",
        "x = base_model.output\n",
        "\n",
        "# Add a global spatial average pooling layer\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# # # Additional Dense layer\n",
        "x = Dense(256, activation='relu')(x)\n",
        "# # #  Help prevent overfitting by randomly setting a fraction of input units to 0 during training\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Add a logistic layer\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=\"adam\", loss = 'binary_crossentropy',\n",
        "                          metrics = ['binary_accuracy', 'mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stg033S6BDUy"
      },
      "outputs": [],
      "source": [
        "# We can see details of MobileNet architecure's details\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSXesTv7BPVG"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Reduce learning rate dynamically during training if there's no improvement\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.05, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Monitor the training metric, and stop training if the metric does not improve\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihiYXp9nLbs4"
      },
      "source": [
        "## Train the model ##\n",
        "\n",
        "**<font size=\"3\">Run this if you decided to use oversampling approach.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moOEhnj1LQvB"
      },
      "outputs": [],
      "source": [
        "# We train for 63 epochs\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=ntrain // batch_size,\n",
        "    epochs=63,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=nval // batch_size,\n",
        "    callbacks=[reduce_lr, early_stopping]  # Add the learning rate scheduler callback\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm-qlt9PLv5X"
      },
      "source": [
        "## Train the model ##\n",
        "\n",
        "**<font size=\"3\">Run this if you decided to use class weights approach.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEfAonJVLTra"
      },
      "outputs": [],
      "source": [
        "# We train for 63 epochs\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=ntrain // batch_size,\n",
        "    epochs=63,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=nval // batch_size,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[reduce_lr, early_stopping]  # Add the learning rate scheduler callback\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8GI3yUxBTvG"
      },
      "source": [
        "## Evaluate the model on the testing data  ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97qL7-k5BUcy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "test_results = model.evaluate(test_generator, steps=ntest // batch_size, verbose=1)\n",
        "print(\"Test Loss:\", test_results[0])\n",
        "print(\"Test Accuracy:\", test_results[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK9IX7RCBZIq"
      },
      "source": [
        "## Visualize the training and validation accuracy and loss ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3Qf38gQBZnJ"
      },
      "outputs": [],
      "source": [
        "# Lets plot the train and val curve\n",
        "# Get the details form the history object\n",
        "acc = history.history['binary_accuracy']\n",
        "val_acc = history.history['val_binary_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "#Train and validation accuracy\n",
        "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
        "plt.title('Training and Validation accurarcy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "#Train and validation loss\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT9TecegBdHE"
      },
      "source": [
        "## Estimation of classification performance / Result ##\n",
        "**<font size=\"3\">We will assess the classification performance of our model using various evaluation metrics. Employing multiple metrics is crucial to thoroughly evaluate the model's correctness and optimization. Our examination will include metrics such as Accuracy, Recall, Precision, F1 score, and AUC score to comprehensively gauge the model's performance. </font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0TNj7KOBdwn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, np.round(preds)) * 100\n",
        "cm = confusion_matrix(y_test, np.round(preds))\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print('CONFUSION MATRIX ------------------')\n",
        "print(cm)\n",
        "\n",
        "print('\\n============TEST METRICS=============')\n",
        "precision = tp/(tp+fp) * 100\n",
        "recall = tp/(tp+fn) * 100\n",
        "print('Accuracy: {}%'.format(acc))\n",
        "print('Precision: {}%'.format(precision))\n",
        "print('Recall: {}%'.format(recall))\n",
        "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
        "\n",
        "print('\\nTRAIN METRIC ----------------------')\n",
        "print('Train acc: {}'.format(np.round((history.history['binary_accuracy'][-1]) * 100, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U66ldLEOBi9l"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfiFCN_OBmOy"
      },
      "source": [
        "## Visualize the ROC ##\n",
        "**<font size=\"3\">The ROC (receiver operating characteristic) curve indicates the diagnostic accuracy and porformance of a model.We show the ROC curve and also calculate AUC score</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTHccNUnBnQs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "fpr , tpr , thresholds = roc_curve ( y_test , preds)\n",
        "auc_keras = auc(fpr, tpr)\n",
        "print(\"AUC Score:\",auc_keras)\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % auc_keras)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgpSyf7HAK-7"
      },
      "source": [
        "## Cross-Validation ##\n",
        "\n",
        "**<font size=\"3\">Cross-validation is a valuable technique for obtaining a robust evaluation of a model's performance, especially when dealing with limited training data. It provides insights into how well the model generalizes to different subsets, helping to make more informed decisions about its suitability for unseen data.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg11h5O-AQdj"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set(style= \"darkgrid\", color_codes = True)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Input, GlobalAveragePooling2D\n",
        "from keras.regularizers import l2\n",
        "from keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "def model():\n",
        "  base_model = MobileNet(weights=None, include_top=False,input_shape=(img_size, img_size, 3))\n",
        "\n",
        "  x = base_model.output\n",
        "\n",
        "  # Add a global spatial average pooling layer\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "  # # # Additional Dense layer\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  # # #  Help prevent overfitting by randomly setting a fraction of input units to 0 during training\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "\n",
        "  # Add a logistic layer\n",
        "  predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  return model\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=7,\n",
        "                             width_shift_range=0.05,\n",
        "                             height_shift_range=0.05,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=False\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize an empty list to store accuracy scores\n",
        "accuracy_scores = []\n",
        "c = 1\n",
        "batch = 16\n",
        "# Iterate through each fold\n",
        "for train_index, test_index in kf.split(X_train, y_train):\n",
        "    X_train_cross, X_test_cross = X_train[train_index], X_train[test_index]\n",
        "    y_train_cross, y_test_cross = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    train_generator = datagen.flow(X_train_cross, y_train_cross, batch_size = batch)\n",
        "\n",
        "     # Initialize and compile the model\n",
        "    model = model()\n",
        "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    print(f\"starting subsequence {c}:\")\n",
        "    # Train the model\n",
        "    model.fit(      train_generator,\n",
        "                    steps_per_epoch=len(X_train_cross) // batch,\n",
        "                    epochs=63,\n",
        "                    callbacks = [reduce_lr, early_stopping]\n",
        "    )\n",
        "    print(f\"finish subsequence {c}\")\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "\n",
        "    evaluation_results1 = model.evaluate(X_test_cross, y_test_cross, verbose = 0)\n",
        "    print(f\"Test loss: {evaluation_results1[0] * 100:.2f}%, Test accuracy: {evaluation_results1[1] * 100:.2f}%\")\n",
        "\n",
        "    y_pred_prob = model.predict(X_test_cross)\n",
        "\n",
        "    # Convert probabilities to binary predictions using a threshold (e.g., 0.5)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_cross, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "    print(f\"\\nThe accuracy for the subsequence {c} is: {accuracy}\")\n",
        "    model.save(f'bagley{c}.h5')\n",
        "    c += 1\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "# Print the cross-validated accuracy\n",
        "print(\"Cross-validated Accuracy: {:.2f}%\".format(np.mean(accuracy_scores) * 100))\n",
        "print(f\"\\nThis is the accuracy for:\\nSubsequence 1 {accuracy_scores[0]}\\nSubsequence 2 {accuracy_scores[1]}\\nSubsequence 3 {accuracy_scores[2]}\\nSubsequence 4 {accuracy_scores[3]}\\nSubsequence 5 {accuracy_scores[4]} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHhjdItTD_vD"
      },
      "source": [
        "## Multi model combination ##\n",
        "\n",
        "**<font size=\"3\">After developing four distinct models, we merge their outcomes to enhance accuracy.\n",
        "The algorithm assesses predictions from each model, returning the most probable result.\n",
        "However, when all four models yield identical predictions for opposing outcomes, we prioritize the best prediction based on the models' class accuracy.</font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxIDQiE-EAKt"
      },
      "outputs": [],
      "source": [
        "# Assuming y_val contains the true labels for the validation set\n",
        "d_fra = {}\n",
        "\n",
        "\n",
        "final_predictions = []\n",
        "final_predictions_fra = []\n",
        "c = 0\n",
        "for i in range(len(predictions_MK54)):\n",
        "  final_predictions.append((predictions_Bagley[i][0] + predictions_MK54[i][0] + predictions_Giovanna[i][0] + predictions_Lola[i][0])/ 4)\n",
        "\n",
        "\n",
        "for i in range(len(predictions_MK54)):\n",
        "  l = [0] * 4\n",
        "  l[0] = predictions_Bagley[i][0]\n",
        "  l[1] = predictions_MK54[i][0]\n",
        "  l[2] = predictions_Giovanna[i][0]\n",
        "  l[3] = predictions_Lola[i][0]\n",
        "\n",
        "  d_fra[i] = l\n",
        "  c = 0\n",
        "  for v in range(len(l)):\n",
        "    if l[v] > 0.5:\n",
        "      c += 1\n",
        "    else:\n",
        "      c -= 1\n",
        "  if c > 0:\n",
        "    final_predictions_fra.append(max(l))\n",
        "  elif c == 0:\n",
        "    if l[0] > 0.5 or l[1] > 0.5 and l[2] < 0.5 or l[3] < 0.5:\n",
        "      final_predictions_fra.append(max(l))\n",
        "    elif l[2] > 0.5 or l[3] > 0.5 and l[0] < 0.5 or l[1] < 0.5:\n",
        "      final_predictions_fra.append(min(l))\n",
        "  else:\n",
        "    final_predictions_fra.append(min(l))\n",
        "\n",
        "\n",
        "\n",
        "final_predictions_array = np.array(final_predictions)\n",
        "final_predictions_combined_models_array = np.array(final_predictions_fra)\n",
        "\n",
        "# print(final_predictions)\n",
        "\n",
        "\n",
        "# Set a threshold for binary classification\n",
        "threshold = 0.5\n",
        "\n",
        "# Round the predicted probabilities to binary predictions\n",
        "predictions_binary_Bagley = (predictions_Bagley > threshold).astype(int)\n",
        "predictions_binary_MK54 = (predictions_MK54 > threshold).astype(int)\n",
        "predictions_binary_Giovanna = (predictions_Giovanna > threshold).astype(int)\n",
        "predictions_binary_Lola = (predictions_Lola > threshold).astype(int)\n",
        "predictions_binary_mean = (final_predictions_array > threshold).astype(int)\n",
        "predictions_binary_combined_models = (final_predictions_combined_models_array > threshold).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a DataFrame for better visualization (optional, requires pandas)\n",
        "import pandas as pd\n",
        "\n",
        "df_Bagley = pd.DataFrame({\n",
        "    'True Label': covid_y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_Bagley.flatten(),\n",
        "    'Predicted Probability': predictions_Bagley.flatten()\n",
        "})\n",
        "\n",
        "df_MK54 = pd.DataFrame({\n",
        "    'True Label': covid_y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_MK54.flatten(),\n",
        "    'Predicted Probability': predictions_MK54.flatten()\n",
        "})\n",
        "\n",
        "df_Giovanna = pd.DataFrame({\n",
        "    'True Label': covid_y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_Giovanna.flatten(),\n",
        "    'Predicted Probability': predictions_Giovanna.flatten()\n",
        "})\n",
        "\n",
        "df_Lola = pd.DataFrame({\n",
        "    'True Label': covid_y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_Lola.flatten(),\n",
        "    'Predicted Probability': predictions_Lola.flatten()\n",
        "})\n",
        "\n",
        "df_mean = pd.DataFrame({\n",
        "    'True Label': covid_y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_mean.flatten(),\n",
        "    'Predicted Probability': final_predictions_array.flatten()\n",
        "})\n",
        "\n",
        "\n",
        "df_combined_models = pd.DataFrame({\n",
        "    'True Label': covid_y_test.flatten(),  # Flatten if y_val is not a 1D array\n",
        "    'Predicted Binary': predictions_binary_combined_models.flatten(),\n",
        "    'Predicted Probability': final_predictions_combined_models_array.flatten()\n",
        "})\n",
        "# Display the DataFrame\n",
        "# print(df)\n",
        "df_Bagley['Predicted Probability (%)'] = (df_Bagley['Predicted Probability']).round(2)\n",
        "df_MK54['Predicted Probability (%)'] = (df_MK54['Predicted Probability']).round(2)\n",
        "df_Giovanna['Predicted Probability (%)'] = (df_Giovanna['Predicted Probability']).round(2)\n",
        "df_Lola['Predicted Probability (%)'] = (df_Lola['Predicted Probability']).round(2)\n",
        "\n",
        "df_mean['Predicted Probability (%)'] = (df_mean['Predicted Probability']).round(2)\n",
        "df_combined_models['Predicted Probability (%)'] = (df_combined_models['Predicted Probability']).round(2)\n",
        "\n",
        "df_Bagley = df_Bagley.drop(columns=['Predicted Probability'])\n",
        "df_MK54 = df_MK54.drop(columns=['Predicted Probability'])\n",
        "df_Giovanna = df_Giovanna.drop(columns=['Predicted Probability'])\n",
        "df_Lola = df_Lola.drop(columns=['Predicted Probability'])\n",
        "\n",
        "df_mean = df_mean.drop(columns=['Predicted Probability'])\n",
        "df_combined_models = df_combined_models.drop(columns=['Predicted Probability'])\n",
        "# print(df.sample(15))\n",
        "\n",
        "# Print only the rows where predictions are incorrect\n",
        "incorrect_predictions_Bagley = df_Bagley[df_Bagley['True Label'] != df_Bagley['Predicted Binary']]\n",
        "incorrect_predictions_MK54 = df_MK54[df_MK54['True Label'] != df_MK54['Predicted Binary']]\n",
        "incorrect_predictions_Giovanna = df_Giovanna[df_Giovanna['True Label'] != df_Giovanna['Predicted Binary']]\n",
        "incorrect_predictions_Lola = df_Lola[df_Lola['True Label'] != df_Lola['Predicted Binary']]\n",
        "\n",
        "incorrect_predictions_mean = df_mean[df_mean['True Label'] != df_mean['Predicted Binary']]\n",
        "incorrect_predictions_combined_models = df_combined_models[df_combined_models['True Label'] != df_combined_models['Predicted Binary']]\n",
        "\n",
        "\n",
        "print(\"Mistakes in prediction Bagley\")\n",
        "print(len(incorrect_predictions_Bagley))\n",
        "\n",
        "print(\"Mistakes in prediction MK54\")\n",
        "print(len(incorrect_predictions_MK54))\n",
        "\n",
        "print(\"Mistakes in prediction Giovanna\")\n",
        "print(len(incorrect_predictions_Giovanna))\n",
        "\n",
        "print(\"Mistakes in prediction Lola\")\n",
        "print(len(incorrect_predictions_Lola))\n",
        "\n",
        "print(\"Mistakes in mean prediction\")\n",
        "print(len(incorrect_predictions_mean))\n",
        "\n",
        "print(\"Mistakes in fra prediction\")\n",
        "print(len(incorrect_predictions_combined_models))\n",
        "\n",
        "print(\"\\nIncorrect Predictions Bagley\")\n",
        "print(incorrect_predictions_Bagley)\n",
        "\n",
        "print(\"\\nIncorrect Predictions MK54\")\n",
        "print(incorrect_predictions_MK54)\n",
        "\n",
        "print(\"\\nIncorrect Predictions Giovanna\")\n",
        "print(incorrect_predictions_Giovanna)\n",
        "\n",
        "print(\"\\nIncorrect Predictions Lola:\")\n",
        "print(incorrect_predictions_Lola)\n",
        "\n",
        "print(\"\\nIncorrect Predictions mean:\")\n",
        "print(incorrect_predictions_mean)\n",
        "\n",
        "print(\"\\nIncorrect Predictions combined_models:\")\n",
        "print(incorrect_predictions_combined_models)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
